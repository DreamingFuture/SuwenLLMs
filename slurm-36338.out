Training Alpaca-LoRA model with params:
base_model: /home/bingxing2/home/scx6503/yx/Linly-Chinese-LLaMA-7B
data_path: /home/bingxing2/home/scx6503/yx/LaWGPT/data/Sample.json
output_dir: ./outputs/train-clm-llama-7B-2
batch_size: 32
micro_batch_size: 2
num_epochs: 800
learning_rate: 0.0003
cutoff_len: 900
val_set_size: 0
lora_r: 8
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj', 'gate_proj', 'down_proj', 'up_proj']
train_on_inputs: True
add_eos_token: True
group_by_length: True
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.55s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:08<00:08,  8.77s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.09s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1,2,none
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it]
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1,2,none
Found cached dataset json (/home/bingxing2/home/scx6503/.cache/huggingface/datasets/json/default-889e96a2ecc785f2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 229.01it/s]
trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2957573965106688
Map:   0%|          | 0/1057 [00:00<?, ? examples/s]Found cached dataset json (/home/bingxing2/home/scx6503/.cache/huggingface/datasets/json/default-889e96a2ecc785f2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 527.52it/s]
trainable params: 19988480 || all params: 6758404096 || trainable%: 0.2957573965106688
Map:   0%|          | 0/1057 [00:00<?, ? examples/s]Map:   3%|▎         | 30/1057 [00:00<00:03, 291.65 examples/s]Map:   3%|▎         | 31/1057 [00:00<00:03, 300.95 examples/s]Map:   6%|▌         | 62/1057 [00:00<00:03, 301.39 examples/s]Map:   6%|▌         | 63/1057 [00:00<00:03, 306.20 examples/s]Map:   9%|▉         | 93/1057 [00:00<00:03, 302.83 examples/s]Map:  10%|█         | 109/1057 [00:00<00:03, 303.00 examples/s]Map:  12%|█▏        | 125/1057 [00:00<00:03, 302.92 examples/s]Map:  13%|█▎        | 142/1057 [00:00<00:02, 307.84 examples/s]Map:  15%|█▍        | 157/1057 [00:00<00:02, 304.16 examples/s]Map:  16%|█▋        | 174/1057 [00:00<00:02, 307.77 examples/s]Map:  19%|█▉        | 203/1057 [00:00<00:02, 300.75 examples/s]Map:  19%|█▉        | 206/1057 [00:00<00:02, 306.57 examples/s]Map:  23%|██▎       | 247/1057 [00:00<00:02, 293.92 examples/s]Map:  24%|██▎       | 251/1057 [00:00<00:02, 300.45 examples/s]Map:  26%|██▌       | 277/1057 [00:00<00:02, 294.19 examples/s]Map:  27%|██▋       | 282/1057 [00:00<00:02, 298.58 examples/s]Map:  29%|██▉       | 307/1057 [00:01<00:02, 293.81 examples/s]Map:  30%|██▉       | 312/1057 [00:01<00:02, 297.99 examples/s]Map:  32%|███▏      | 338/1057 [00:01<00:02, 295.51 examples/s]Map:  32%|███▏      | 343/1057 [00:01<00:02, 297.11 examples/s]Map:  35%|███▌      | 374/1057 [00:01<00:02, 297.43 examples/s]Map:  36%|███▌      | 382/1057 [00:01<00:02, 293.61 examples/s]Map:  38%|███▊      | 405/1057 [00:01<00:02, 298.51 examples/s]Map:  39%|███▉      | 412/1057 [00:01<00:02, 292.14 examples/s]Map:  41%|████      | 436/1057 [00:01<00:02, 298.56 examples/s]Map:  42%|████▏     | 443/1057 [00:01<00:02, 293.70 examples/s]Map:  44%|████▍     | 467/1057 [00:01<00:01, 298.30 examples/s]Map:  45%|████▍     | 474/1057 [00:01<00:01, 293.80 examples/s]Map:  48%|████▊     | 512/1057 [00:01<00:01, 296.01 examples/s]Map:  49%|████▉     | 519/1057 [00:01<00:01, 292.12 examples/s]Map:  51%|█████▏    | 543/1057 [00:01<00:01, 296.08 examples/s]Map:  52%|█████▏    | 550/1057 [00:01<00:01, 293.64 examples/s]Map:  54%|█████▍    | 574/1057 [00:01<00:01, 297.19 examples/s]Map:  55%|█████▍    | 580/1057 [00:01<00:01, 292.97 examples/s]Map:  57%|█████▋    | 606/1057 [00:02<00:01, 299.36 examples/s]Map:  58%|█████▊    | 611/1057 [00:02<00:01, 294.53 examples/s]Map:  61%|██████    | 641/1057 [00:02<00:01, 292.45 examples/s]Map:  61%|██████▏   | 650/1057 [00:02<00:01, 294.90 examples/s]Map:  64%|██████▍   | 680/1057 [00:02<00:01, 293.55 examples/s]Map:  65%|██████▍   | 686/1057 [00:02<00:01, 290.35 examples/s]Map:  67%|██████▋   | 711/1057 [00:02<00:01, 294.78 examples/s]Map:  68%|██████▊   | 716/1057 [00:02<00:01, 290.69 examples/s]Map:  70%|███████   | 742/1057 [00:02<00:01, 296.47 examples/s]Map:  71%|███████   | 746/1057 [00:02<00:01, 289.96 examples/s]Map:  73%|███████▎  | 776/1057 [00:02<00:00, 290.45 examples/s]Map:  74%|███████▍  | 786/1057 [00:02<00:00, 293.93 examples/s]Map:  76%|███████▋  | 806/1057 [00:02<00:00, 288.52 examples/s]Map:  77%|███████▋  | 818/1057 [00:02<00:00, 296.39 examples/s]Map:  79%|███████▉  | 836/1057 [00:02<00:00, 290.36 examples/s]Map:  80%|████████  | 849/1057 [00:02<00:00, 296.14 examples/s]Map:  83%|████████▎ | 881/1057 [00:03<00:00, 291.15 examples/s]Map:  85%|████████▍ | 894/1057 [00:03<00:00, 294.18 examples/s]Map:  86%|████████▌ | 911/1057 [00:03<00:00, 290.21 examples/s]Map:  89%|████████▉ | 939/1057 [00:03<00:00, 292.15 examples/s]Map:  89%|████████▉ | 941/1057 [00:03<00:00, 290.16 examples/s]Map:  92%|█████████▏| 970/1057 [00:03<00:00, 292.93 examples/s]Map:  92%|█████████▏| 971/1057 [00:03<00:00, 291.51 examples/s]Map:  95%|█████████▍| 1000/1057 [00:03<00:00, 179.44 examples/s]Map:  96%|█████████▌| 1015/1057 [00:03<00:00, 179.03 examples/s]Map:  98%|█████████▊| 1032/1057 [00:03<00:00, 203.37 examples/s]Map:  99%|█████████▉| 1048/1057 [00:03<00:00, 203.81 examples/s]                                                                                                                                wandb: Tracking run with wandb version 0.15.3
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|          | 0/26400 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/bingxing2/home/scx6503/yx/LaWGPT/train_clm.py", line 259, in <module>
    fire.Fire(train)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/fire/core.py", line 466, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/bingxing2/home/scx6503/yx/LaWGPT/train_clm.py", line 251, in train
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 1664, in train
    return inner_training_loop(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 2735, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 2767, in compute_loss
    outputs = model(**inputs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/peft/peft_model.py", line 678, in forward
    return self.base_model(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 688, in forward
    Traceback (most recent call last):
outputs = self.model(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/home/bingxing2/home/scx6503/yx/LaWGPT/train_clm.py", line 259, in <module>
    fire.Fire(train)
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/fire/core.py", line 466, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
      File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 578, in forward
  File "/home/bingxing2/home/scx6503/yx/LaWGPT/train_clm.py", line 251, in train
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 1664, in train
    return inner_training_loop(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 1938, in _inner_training_loop
    tr_loss_step = self.training_step(model, inputs)
    layer_outputs = decoder_layer(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 2735, in training_step
    loss = self.compute_loss(model, inputs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/trainer.py", line 2767, in compute_loss
    outputs = model(**inputs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1040, in forward
    output = self._run_ddp_forward(*inputs, **kwargs)
      File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 1000, in _run_ddp_forward
    return module_to_run(*inputs[0], **kwargs[0])
return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/peft/peft_model.py", line 678, in forward
    return self.base_model(
    output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 293, in forward
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 688, in forward
    outputs = self.model(
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 578, in forward
    layer_outputs = decoder_layer(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
    return forward_call(*input, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 293, in forward
    hidden_states, self_attn_weights, present_key_value = self.self_attn(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
    output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/accelerate/hooks.py", line 165, in new_forward
    output = old_forward(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 232, in forward
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py", line 232, in forward
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 0; 39.41 GiB total capacity; 37.45 GiB already allocated; 78.50 MiB free; 38.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 100.00 MiB (GPU 1; 39.41 GiB total capacity; 37.45 GiB already allocated; 78.50 MiB free; 38.12 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
wandb: Waiting for W&B process to finish... (failed 1).
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/bingxing2/home/scx6503/yx/LaWGPT/wandb/offline-run-20230712_202556-v7m9z668
wandb: Find logs at: ./wandb/offline-run-20230712_202556-v7m9z668/logs
WARNING:torch.distributed.elastic.multiprocessing.api:Sending process 50503 closing signal SIGTERM
ERROR:torch.distributed.elastic.multiprocessing.api:failed (exitcode: 1) local_rank: 1 (pid: 50504) of binary: /home/bingxing2/home/scx6503/.conda/envs/llama_etuning/bin/python
Traceback (most recent call last):
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 346, in wrapper
    return f(*args, **kwargs)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/distributed/run.py", line 762, in main
    run(args)
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/distributed/run.py", line 753, in run
    elastic_launch(
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 132, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/bingxing2/home/scx6503/.conda/envs/llama_etuning/lib/python3.10/site-packages/torch/distributed/launcher/api.py", line 246, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
train_clm.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2023-07-12_20:26:07
  host      : paraai-n32-h-01-agent-44.paraai-n32-h-01.com
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 50504)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
