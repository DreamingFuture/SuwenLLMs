/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: /home/bingxing2/home/scx6503/.conda/envs/py39 did not contain libcudart.so as expected! Searching further paths...
  warn(msg)
/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:136: UserWarning: WARNING: The following directories listed in your path were found to be non-existent: {PosixPath('/home/bingxing2/apps/cuda/11.6.0/libnvvp'), PosixPath('/home/bingxing2/apps/cuda/11.6.0/libnsight')}
  warn(msg)
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please submit your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
CUDA SETUP: CUDA runtime path found: /home/bingxing2/apps/cuda/11.6.0/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.0
CUDA SETUP: Detected CUDA version 116
CUDA SETUP: Loading binary /home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda116.so...
Training Alpaca-LoRA model with params:
base_model: /home/bingxing2/home/scx6503/yx/Linly-Chinese-LLaMA-7B
data_path: /home/bingxing2/home/scx6503/yx/LaWGPT/data/Sample.json
output_dir: ./outputs/train-clm-llama-7B-2
batch_size: 32
micro_batch_size: 2
num_epochs: 800
learning_rate: 0.0003
cutoff_len: 500
val_set_size: 0
lora_r: 8
lora_alpha: 32
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']
train_on_inputs: True
add_eos_token: True
group_by_length: True
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:03<00:03,  3.63s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.30s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.50s/it]
Found cached dataset json (/home/bingxing2/home/scx6503/.cache/huggingface/datasets/json/default-889e96a2ecc785f2/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
pre-trained model's BOS EOS and PAD token id: 1 2 None  => It should be 1,2,none
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 456.10it/s]
trainable params: 8388608 || all params: 6746804224 || trainable%: 0.12433454005023165
Map:   0%|          | 0/1057 [00:00<?, ? examples/s]Map:   4%|▎         | 39/1057 [00:00<00:02, 376.45 examples/s]Map:   7%|▋         | 79/1057 [00:00<00:02, 382.46 examples/s]Map:  11%|█▏        | 119/1057 [00:00<00:02, 382.29 examples/s]Map:  15%|█▌        | 159/1057 [00:00<00:02, 382.09 examples/s]Map:  19%|█▊        | 198/1057 [00:00<00:02, 381.04 examples/s]Map:  22%|██▏       | 237/1057 [00:00<00:02, 379.57 examples/s]Map:  26%|██▌       | 275/1057 [00:00<00:02, 378.91 examples/s]Map:  30%|██▉       | 313/1057 [00:00<00:01, 376.80 examples/s]Map:  33%|███▎      | 352/1057 [00:00<00:01, 377.57 examples/s]Map:  37%|███▋      | 391/1057 [00:01<00:01, 377.42 examples/s]Map:  41%|████      | 429/1057 [00:01<00:01, 376.72 examples/s]Map:  44%|████▍     | 467/1057 [00:01<00:01, 376.34 examples/s]Map:  48%|████▊     | 506/1057 [00:01<00:01, 377.51 examples/s]Map:  51%|█████▏    | 544/1057 [00:01<00:01, 375.54 examples/s]Map:  55%|█████▌    | 583/1057 [00:01<00:01, 376.65 examples/s]Map:  59%|█████▉    | 621/1057 [00:01<00:01, 376.98 examples/s]Map:  62%|██████▏   | 659/1057 [00:01<00:01, 374.02 examples/s]Map:  66%|██████▌   | 697/1057 [00:01<00:00, 372.67 examples/s]Map:  70%|██████▉   | 735/1057 [00:01<00:00, 372.13 examples/s]Map:  73%|███████▎  | 774/1057 [00:02<00:00, 374.08 examples/s]Map:  78%|███████▊  | 828/1057 [00:02<00:00, 363.90 examples/s]Map:  82%|████████▏ | 866/1057 [00:02<00:00, 366.16 examples/s]Map:  86%|████████▌ | 905/1057 [00:02<00:00, 369.98 examples/s]Map:  91%|█████████ | 960/1057 [00:02<00:00, 366.24 examples/s]Map:  94%|█████████▍| 998/1057 [00:02<00:00, 366.12 examples/s]Map:  98%|█████████▊| 1041/1057 [00:02<00:00, 277.27 examples/s]                                                                wandb: Tracking run with wandb version 0.15.3
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
  0%|          | 0/26400 [00:00<?, ?it/s]Traceback (most recent call last):
  File "/home/bingxing2/home/scx6503/yx/LaWGPT/train_clm.py", line 260, in <module>
    fire.Fire(train)
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/fire/core.py", line 141, in Fire
    component_trace = _Fire(component, args, parsed_flag_args, context, name)
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/fire/core.py", line 466, in _Fire
    component, remaining_args = _CallAndUpdateTrace(
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/fire/core.py", line 681, in _CallAndUpdateTrace
    component = fn(*varargs, **kwargs)
  File "/home/bingxing2/home/scx6503/yx/LaWGPT/train_clm.py", line 252, in train
    trainer.train(resume_from_checkpoint=resume_from_checkpoint)
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/transformers/trainer.py", line 1664, in train
    return inner_training_loop(
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/transformers/trainer.py", line 1973, in _inner_training_loop
    self.scaler.unscale_(self.optimizer)
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 282, in unscale_
    optimizer_state["found_inf_per_device"] = self._unscale_grads_(optimizer, inv_scale, found_inf, False)
  File "/home/bingxing2/home/scx6503/.conda/envs/py39/lib/python3.9/site-packages/torch/cuda/amp/grad_scaler.py", line 210, in _unscale_grads_
    raise ValueError("Attempting to unscale FP16 gradients.")
ValueError: Attempting to unscale FP16 gradients.
wandb: Waiting for W&B process to finish... (failed 1).
wandb: You can sync this run to the cloud by running:
wandb: wandb sync /home/bingxing2/home/scx6503/yx/LaWGPT/wandb/offline-run-20230712_193124-visa89g5
wandb: Find logs at: ./wandb/offline-run-20230712_193124-visa89g5/logs
