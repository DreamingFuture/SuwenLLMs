[2023-07-24 09:09:23,883] [INFO] [real_accelerator.py:110:get_accelerator] Setting ds_accelerator to cuda (auto detect)
/home/yuxiang/anaconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/cuda_setup/main.py:149: UserWarning: /home/yuxiang/anaconda3/envs/peft did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...
  warn(msg)
The model weights are not tied. Please use the `tie_weights` method before using the `infer_auto_device` function.

===================================BUG REPORT===================================
Welcome to bitsandbytes. For bug reports, please run

python -m bitsandbytes

 and submit this information together with your error trace to: https://github.com/TimDettmers/bitsandbytes/issues
================================================================================
bin /home/yuxiang/anaconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so
CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...
CUDA SETUP: CUDA runtime path found: /usr/local/cuda/lib64/libcudart.so
CUDA SETUP: Highest compute capability among GPUs detected: 8.6
CUDA SETUP: Detected CUDA version 121
CUDA SETUP: Loading binary /home/yuxiang/anaconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/libbitsandbytes_cuda121.so...
Training Alpaca-LoRA model with params:
base_model: /data2/yuxiang/data/LLMs/Linly-Chinese-LLaMA-7b-hf
data_path: ./resources/event4_instruction_tune.json
output_dir: ./tuned/train-clm-llama-7B-full-event4
batch_size: 64
micro_batch_size: 8
num_epochs: 2
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 0
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: False
add_eos_token: True
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: alpaca

Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:36<00:36, 36.20s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 16.32s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:38<00:00, 19.30s/it]
Found cached dataset json (/home/yuxiang/.cache/huggingface/datasets/json/default-688de5190ba00bbb/0.0.0/e347ab1c932092252e717ff3f949105a4dd28b27e842dd53157d2f72e276c2e4)
  0%|          | 0/1 [00:00<?, ?it/s]100%|██████████| 1/1 [00:00<00:00, 556.79it/s]
trainable params: 4194304 || all params: 6742609920 || trainable%: 0.06220594176090199
Map:   0%|          | 0/1365 [00:00<?, ? examples/s]Map:   8%|▊         | 106/1365 [00:00<00:01, 1042.51 examples/s]Map:  16%|█▌        | 215/1365 [00:00<00:01, 1065.01 examples/s]Map:  24%|██▍       | 325/1365 [00:00<00:00, 1076.63 examples/s]Map:  32%|███▏      | 435/1365 [00:00<00:00, 1084.18 examples/s]Map:  40%|███▉      | 545/1365 [00:00<00:00, 1087.54 examples/s]Map:  48%|████▊     | 659/1365 [00:00<00:00, 1100.90 examples/s]Map:  56%|█████▋    | 771/1365 [00:00<00:00, 1105.49 examples/s]Map:  69%|██████▊   | 937/1365 [00:00<00:00, 1104.37 examples/s]Map:  78%|███████▊  | 1068/1365 [00:01<00:00, 1014.40 examples/s]Map:  87%|████████▋ | 1182/1365 [00:01<00:00, 1044.02 examples/s]Map:  95%|█████████▍| 1296/1365 [00:01<00:00, 1065.34 examples/s]                                                                   0%|          | 0/42 [00:00<?, ?it/s]/home/yuxiang/anaconda3/envs/peft/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:318: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization
  warnings.warn(f"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization")
  2%|▏         | 1/42 [00:18<12:38, 18.50s/it]  5%|▍         | 2/42 [00:33<11:08, 16.72s/it]  7%|▋         | 3/42 [00:50<10:40, 16.43s/it] 10%|▉         | 4/42 [01:05<10:11, 16.10s/it] 12%|█▏        | 5/42 [01:21<09:49, 15.94s/it] 14%|█▍        | 6/42 [01:37<09:36, 16.02s/it] 17%|█▋        | 7/42 [01:53<09:17, 15.94s/it] 19%|█▉        | 8/42 [02:09<09:01, 15.91s/it] 21%|██▏       | 9/42 [02:25<08:48, 16.01s/it] 24%|██▍       | 10/42 [02:41<08:30, 15.96s/it]                                                24%|██▍       | 10/42 [02:41<08:30, 15.96s/it] 26%|██▌       | 11/42 [02:56<08:11, 15.84s/it] 29%|██▊       | 12/42 [03:12<07:53, 15.79s/it] 31%|███       | 13/42 [03:28<07:42, 15.96s/it] 33%|███▎      | 14/42 [03:44<07:25, 15.90s/it] 36%|███▌      | 15/42 [04:00<07:07, 15.82s/it] 38%|███▊      | 16/42 [04:16<06:57, 16.07s/it] 40%|████      | 17/42 [04:32<06:41, 16.06s/it] 43%|████▎     | 18/42 [04:48<06:24, 16.04s/it] 45%|████▌     | 19/42 [05:04<06:07, 15.96s/it] 48%|████▊     | 20/42 [05:20<05:49, 15.89s/it]                                                48%|████▊     | 20/42 [05:20<05:49, 15.89s/it] 50%|█████     | 21/42 [05:36<05:34, 15.91s/it] 52%|█████▏    | 22/42 [05:51<05:12, 15.62s/it] 55%|█████▍    | 23/42 [06:07<04:57, 15.67s/it] 57%|█████▋    | 24/42 [06:22<04:43, 15.74s/it] 60%|█████▉    | 25/42 [06:39<04:31, 15.98s/it] 62%|██████▏   | 26/42 [06:55<04:14, 15.94s/it] 64%|██████▍   | 27/42 [07:11<03:58, 15.92s/it] 67%|██████▋   | 28/42 [07:27<03:43, 15.98s/it] 69%|██████▉   | 29/42 [07:43<03:28, 16.03s/it] 71%|███████▏  | 30/42 [07:59<03:12, 16.01s/it]                                                71%|███████▏  | 30/42 [07:59<03:12, 16.01s/it] 74%|███████▍  | 31/42 [08:15<02:55, 15.98s/it] 76%|███████▌  | 32/42 [08:30<02:38, 15.85s/it] 79%|███████▊  | 33/42 [08:46<02:22, 15.82s/it] 81%|████████  | 34/42 [09:02<02:06, 15.87s/it] 83%|████████▎ | 35/42 [09:18<01:51, 15.87s/it] 86%|████████▌ | 36/42 [09:34<01:34, 15.78s/it] 88%|████████▊ | 37/42 [09:49<01:18, 15.70s/it] 90%|█████████ | 38/42 [10:05<01:03, 15.77s/it] 93%|█████████▎| 39/42 [10:21<00:47, 15.80s/it] 95%|█████████▌| 40/42 [10:37<00:31, 15.75s/it]                                                95%|█████████▌| 40/42 [10:37<00:31, 15.75s/it] 98%|█████████▊| 41/42 [10:52<00:15, 15.74s/it]100%|██████████| 42/42 [11:08<00:00, 15.84s/it]                                               100%|██████████| 42/42 [11:08<00:00, 15.84s/it]100%|██████████| 42/42 [11:08<00:00, 15.92s/it]
{'loss': 3.1856, 'learning_rate': 0.00025945945945945944, 'epoch': 0.47}
{'loss': 1.7035, 'learning_rate': 0.00017837837837837839, 'epoch': 0.94}
{'loss': 0.9712, 'learning_rate': 9.72972972972973e-05, 'epoch': 1.4}
{'loss': 0.8851, 'learning_rate': 1.6216216216216215e-05, 'epoch': 1.87}
{'train_runtime': 669.4991, 'train_samples_per_second': 4.078, 'train_steps_per_second': 0.063, 'train_loss': 1.6472733929043724, 'epoch': 1.96}

 If there's a warning about missing keys above, please disregard :)
